# Area under ROC Curve (or AUC for short) is a performance metric for binary classification 
# problems. The AUC represents a modelâ€™s ability to discriminate between positive and 
# negative classes. An area of 1.0 represents a model that made all predictions perfectly. 
# An area of 0.5 represents a model that is as good as random. Learn more about ROC here. 
# ROC can be broken down into sensitivity and specificity. A binary classification problem 
# is really a trade-off between sensitivity and specificity.

# Sensitivity is the true positive rate also called the recall. It is the number of instances
# from the positive (first) class that actually predicted correctly.
# Specificity is also called the true negative rate. Is the number of instances from the 
# negative (second) class that were actually predicted correctly.


# Cross Validation Classification ROC AUC
import pandas
from sklearn import cross_validation
from sklearn.linear_model import LogisticRegression
url = "https://goo.gl/vhm1eU"
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] 
dataframe = pandas.read_csv(url, names=names)
array = dataframe.values
X = array[:,0:8]
Y = array[:,8]
num_folds = 10
num_instances = len(X)
seed = 7
kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed) 
model = LogisticRegression()
scoring = 'roc_auc'
results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring=scoring) 
print("AUC: %.3f (%.3f)") % (results.mean(), results.std())
