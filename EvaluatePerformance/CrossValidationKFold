# The choice of k must allow the size of each test partition to be large enough to be a reasonable 
# sample of the problem, whilst allowing enough repetitions of the train-test evaluation of the 
# algorithm to provide a fair estimate of the algorithms performance on unseen data. For modest 
# sized datasets in the thousands or tens of thousands of records, k values of 3, 5 and 10 are common. 
# In the example below we use 10-fold cross validation.

# Evaluate using Cross Validation
import pandas
from sklearn import cross_validation
from sklearn.linear_model import LogisticRegression
url = "https://goo.gl/vhm1eU"
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] 
dataframe = pandas.read_csv(url, names=names)
array = dataframe.values
X = array[:,0:8]
Y = array[:,8]
num_folds = 10
num_instances = len(X)
seed = 7
kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed) 
model = LogisticRegression()
results = cross_validation.cross_val_score(model, X, Y, cv=kfold)
print("Accuracy: %.3f%% (%.3f%%)") % (results.mean()*100.0, results.std()*100.0)
