# You can configure cross validation so that the size of the fold is 1 (k is set to the number 
# of observations in your dataset). This variation of cross validation is called leave-one-out 
# cross validation. The result is a large number of performance measures that can be summarized in
# an effort to give a more reasonable estimate of the accuracy of your model on unseen data. A 
# downside is that it can be a computationally more expensive procedure than k-fold cross validation. 
# In the example below we use leave-one-out cross validation.

# NEEDS MORE/BETTER EXPLANATION THAN ABOVE NONSENSE...


# Evaluate using Leave One Out Cross Validation
import pandas
from sklearn import cross_validation
from sklearn.linear_model import LogisticRegression
url = "https://goo.gl/vhm1eU"
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] 
dataframe = pandas.read_csv(url, names=names)
array = dataframe.values
X = array[:,0:8]
Y = array[:,8]
num_folds = 10
num_instances = len(X)
loocv = cross_validation.LeaveOneOut(n=num_instances)
model = LogisticRegression()
results = cross_validation.cross_val_score(model, X, Y, cv=loocv) 
print("Accuracy: %.3f%% (%.3f%%)") % (results.mean()*100.0, results.std()*100.0)
