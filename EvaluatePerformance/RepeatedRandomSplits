# Another variation on k-fold cross validation is to create a random split of the data 
# like the train/test split described above, but repeat the process of splitting and 
# evaluation of the algorithm multiple times, like cross validation. This has the 
# speed of using a train/test split and the reduction in variance in the estimated 
# performance of k-fold cross validation. You can also repeat the process many more 
# times as needed to improve the accuracy. A down side is that repetitions may include 
# much of the same data in the train or the test split from run to run, introducing 
# redundancy into the evaluation. The example below splits the data into a 67%/33% 
# train/test split and repeats the process 10 times.

# Evaluate using Shuffle Split Cross Validation
import pandas
from sklearn import cross_validation
from sklearn.linear_model import LogisticRegression
url = "https://goo.gl/vhm1eU"
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] 
dataframe = pandas.read_csv(url, names=names)
array = dataframe.values
X = array[:,0:8]
Y = array[:,8]
num_samples = 10
test_size = 0.33
num_instances = len(X)
seed = 7
kfold = cross_validation.ShuffleSplit(n=num_instances, n_iter=num_samples,test_size=test_size, random_state=seed)
model = LogisticRegression()
results = cross_validation.cross_val_score(model, X, Y, cv=kfold)
print("Accuracy: %.3f%% (%.3f%%)") % (results.mean()*100.0, results.std()*100.0)
